{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d4f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3e6b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/vkp7rrvd55g8tqrlq2_hlq2w0000gn/T/ipykernel_47249/2283391567.py:6: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  game_df.fillna(game_df.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "game_df = pd.read_csv(os.path.join('csv','game.csv'))\n",
    "# player_df = pd.read_csv('player.csv')\n",
    "# other_stats_df = pd.read_csv('other_stats.csv')\n",
    "game_features = ['season_id', 'team_id_home', 'team_abbreviation_home', 'pts_home', 'team_id_away', 'team_abbreviation_away', 'pts_away']\n",
    "game_df = game_df[game_features]\n",
    "game_df.fillna(game_df.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cba72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "game_df['team_abbreviation_home'] = encoder.fit_transform(game_df['team_abbreviation_home'])\n",
    "game_df['team_abbreviation_away'] = encoder.fit_transform(game_df['team_abbreviation_away'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d044b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "game_df[['pts_home', 'pts_away']] = scaler.fit_transform(game_df[['pts_home', 'pts_away']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5de7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(game_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = data[['team_abbreviation_home', 'team_abbreviation_away', 'pts_home', 'pts_away']].values\n",
    "        self.y = data['season_id'].values  # This should be the target variable you are interested in\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763ff4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SportsDataset(train_df)\n",
    "val_dataset = SportsDataset(val_df)\n",
    "test_dataset = SportsDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce7470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "# Assuming the inputs from game features are all numeric after encoding and scaling\n",
    "input_size = len(train_dataset[0][0])  # This should match the number of features\n",
    "hidden_size = 50  # Example size, can be adjusted\n",
    "num_classes = len(torch.unique(train_dataset[:][1]))  # Adjust based on your target\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e4f2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `season_id` is your target and exists in your train, val, and test sets\n",
    "all_labels = np.concatenate([train_df['season_id'], val_df['season_id'], test_df['season_id']])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "train_df['season_id'] = label_encoder.transform(train_df['season_id'])\n",
    "val_df['season_id'] = label_encoder.transform(val_df['season_id'])\n",
    "test_df['season_id'] = label_encoder.transform(test_df['season_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f297043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_encoder.classes_)  # Adjust this based on remapped labels\n",
    "model = MLP(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54cd00ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([64, 4]) Target shape: torch.Size([64])\n",
      "Target sample: tensor([22003, 22008, 21995, 21995, 31963])\n"
     ]
    }
   ],
   "source": [
    "# Checking the data loader output\n",
    "for data, target in train_loader:\n",
    "    print(\"Data shape:\", data.shape, \"Target shape:\", target.shape)\n",
    "    print(\"Target sample:\", target[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0b420de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate validation accuracy after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            print(f'Epoch {epoch+1}: Validation Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a182e945",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 22003 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Set the number of epochs based on how training progresses, start small to prevent long crashes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 35\u001b[0m train_model(model, train_loader, val_loader, num_epochs)\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 22003 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification problems\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            print(f'Validation Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Set the number of epochs based on how training progresses, start small to prevent long crashes\n",
    "num_epochs = 5\n",
    "train_model(model, train_loader, val_loader, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9db7b2",
   "metadata": {},
   "source": [
    "BASE MODEL CONSTRUCTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a10a877",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 22009 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (features, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m---> 36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 22009 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Model Hyperparameters\n",
    "input_size = 4  # Based on the number of features used from the dataset\n",
    "hidden_size = 64  # A moderate size for hidden layer\n",
    "num_classes = len(game_df['season_id'].unique())  # Based on unique season IDs\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Combines softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Validate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in val_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Validation Accuracy of the model on the validation set: {100 * correct / total}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1985406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a6a864",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 21982 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy of the model on the validation set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m train_model(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (features, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 21982 is out of bounds."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612b8fb",
   "metadata": {},
   "source": [
    "Ver 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b49f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d403d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class SimpleMLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(SimpleMLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to hidden layer\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)  # Hidden to output layer\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))  # Activation function for hidden layer\n",
    "#         x = self.fc2(x)  # No activation for output layer, assuming regression or raw output needed\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1135cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_dim = 4  # Adjust based on the number of input features\n",
    "hidden_dim = 32  # Modest number of hidden nodes\n",
    "output_dim = 1  # Adjust based on your specific prediction task (regression or classification)\n",
    "\n",
    "\n",
    "# Model initialization\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f78f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features = features.float()  # Ensure features are in float\n",
    "            targets = targets.long()  # Convert targets to float if using MSE or similar\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients for this training step\n",
    "            outputs = model(features)  # Forward pass\n",
    "\n",
    "            # Ensure outputs and targets are the correct shape\n",
    "            outputs = outputs.squeeze()  # Remove any extra dimensions from outputs\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Apply gradients\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation loss\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(criterion(model(features.float()).squeeze(), targets.float()) for features, targets in val_loader) / len(val_loader)\n",
    "        print(f'Validation Loss after Epoch [{epoch+1}/{epochs}]: {valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd25c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50):\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()  # Set the model to training mode\n",
    "#         for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "#             optimizer.zero_grad()  # Clear gradients for this training step\n",
    "#             outputs = model(features)  # Forward pass\n",
    "#             loss = criterion(outputs, targets)  # Compute loss\n",
    "#             loss.backward()  # Backpropagation\n",
    "#             optimizer.step()  # Apply gradients\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "#         # Validation loss\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         with torch.no_grad():\n",
    "#             valid_loss = sum(criterion(model(features), targets) for features, targets in val_loader) / len(val_loader)\n",
    "#         print(f'Validation Loss after Epoch [{epoch+1}/{epochs}]: {valid_loss:.4f}')\n",
    "\n",
    "# # Call to train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eab5a5",
   "metadata": {},
   "source": [
    "ver 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a421a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()                         # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a832ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4  # Number of input features\n",
    "hidden_size = 16  # Number of hidden units\n",
    "num_classes = 10  # Example: number of season types or classifications\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5821882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52261ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d7d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19498478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.layer1(x))\n",
    "        out = self.relu2(self.layer2(out))\n",
    "        out = self.relu3(self.layer3(out))\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd346e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4  # Adjust this based on the number of features your dataset has\n",
    "num_classes = 10  # Adjust based on the number of output classes\n",
    "\n",
    "model = EnhancedMLP(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58707c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a09d93",
   "metadata": {},
   "source": [
    "VER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to load for each dataframe to minimize memory usage\n",
    "# game_cols = ['game_id', 'team_id_home', 'team_id_away', 'team_abbreviation_home', 'team_abbreviation_away', 'pts_home', 'season_type', 'pts_away']\n",
    "# player_cols = ['id', 'full_name']  # Adjust based on actual usage needs\n",
    "# other_stats_cols = ['game_id', 'team_id_home', 'team_id_away', 'pts_paint_home', 'pts_paint_away', 'total_turnovers_home', 'total_turnovers_away']\n",
    "# line_score_cols = ['game_id', 'team_id_home', 'team_id_away', 'pts_home', 'pts_away']\n",
    "# game_features = ['season_id', 'team_id_home', 'team_abbreviation_home', 'pts_home', 'team_id_away', 'team_abbreviation_away', 'pts_away']\n",
    "# game_df = game_df[game_features]\n",
    "\n",
    "\n",
    "# # Load datasets with selected columns\n",
    "# game_df = pd.read_csv('game.csv', usecols=game_cols)\n",
    "# player_df = pd.read_csv('player.csv', usecols=player_cols)\n",
    "# other_stats_df = pd.read_csv('other_stats.csv', usecols=other_stats_cols)\n",
    "# line_score_df = pd.read_csv('line_score.csv', usecols=line_score_cols)\n",
    "# Load datasets\n",
    "#game_df = pd.read_csv(os.path.join('csv','game.csv'), usecols=game_cols)\n",
    "# player_df = pd.read_csv(os.path.join('csv','player.csv'), usecols=player_cols)\n",
    "# other_stats_df = pd.read_csv(os.path.join('csv','other_stats.csv'), usecols=other_stats_cols)\n",
    "# line_score_df = pd.read_csv(os.path.join('csv','line_score.csv'), usecols=line_score_cols)\n",
    "\n",
    "# # Merge datasets based on 'game_id' and 'team_id_home'/'team_id_away'\n",
    "# merged_df = pd.merge(game_df, other_stats_df, on=[\"game_id\", \"team_id_home\", \"team_id_away\"])\n",
    "# merged_df = pd.merge(merged_df, line_score_df, on=[\"game_id\", \"team_id_home\", \"team_id_away\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba26622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'pts_home_x' refers to the game file and 'pts_home_y' refers to the line score file, and they should be identical\n",
    "# assert (merged_df['pts_home_x'] == merged_df['pts_home_y']).all(), \"Point columns differ and need inspection.\"\n",
    "\n",
    "# # Drop the redundant column if they are identical\n",
    "# merged_df.drop('pts_home_y', axis=1, inplace=True)\n",
    "# merged_df.rename(columns={'pts_home_x': 'pts_home'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical columns with the mean\n",
    "numerical_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "merged_df[numerical_cols] = merged_df[numerical_cols].fillna(merged_df[numerical_cols].mean())\n",
    "\n",
    "# Fill categorical columns with the mode\n",
    "categorical_cols = merged_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    merged_df[col] = merged_df[col].fillna(merged_df[col].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define columns to be encoded and scaled\n",
    "# categorical_features = ['team_abbreviation_home', 'team_abbreviation_away', 'season_type']\n",
    "# numerical_features = list(numerical_cols)\n",
    "# numerical_features.remove('pts_home')  # Remove target variable from scaling\n",
    "\n",
    "# # Create a transformer for scaling and encoding\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numerical_features),\n",
    "#         ('cat', OneHotEncoder(), categorical_features)\n",
    "#     ])\n",
    "\n",
    "# # Configure pipeline with preprocessing and a dummy estimator\n",
    "# pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# # Fit and transform the data\n",
    "# X_transformed = pipe.fit_transform(merged_df.drop('pts_home', axis=1))\n",
    "# y = merged_df['pts_home'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cce4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for encoding and scaling\n",
    "categorical_features = ['team_abbreviation_home', 'team_abbreviation_away', 'season_type']\n",
    "numerical_features = [col for col in numerical_cols if col != 'pts_home']  # exclude target variable from features\n",
    "\n",
    "# Create the transformer and pipeline for handling categorical and numerical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(sparse_output=True), categorical_features),  # Maintain sparse format to save memory\n",
    "    ], \n",
    "    sparse_threshold=0.3  # This ensures the output will be sparse if less than 30% of the data is nonzero\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "X_transformed = pipe.fit_transform(merged_df.drop('pts_home', axis=1))\n",
    "y = merged_df['pts_home'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3393ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is in dense format if necessary and convert to PyTorch tensors\n",
    "if isinstance(X_transformed, np.ndarray):\n",
    "    X_tensor = torch.tensor(X_transformed.astype(np.float32))\n",
    "else:\n",
    "    # Only convert to dense if it is necessary (sparse matrices can be handled directly depending on the model)\n",
    "    X_tensor = torch.tensor(X_transformed.toarray().astype(np.float32))\n",
    "\n",
    "y_tensor = torch.tensor(y.astype(np.float32))\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, temp_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "valid_dataset, test_dataset = train_test_split(temp_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders with a smaller batch size\n",
    "batch_size = 32  # Reduced batch size from 64 to 32 to manage memory usage better\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the sparse matrix to a dense matrix/array before converting to PyTorch tensors\n",
    "# X_dense = X_transformed.todense() if hasattr(X_transformed, 'todense') else X_transformed\n",
    "# X_tensor = torch.tensor(X_dense.astype(np.float32))\n",
    "# y_tensor = torch.tensor(y.astype(np.float32))\n",
    "\n",
    "# # Create a TensorDataset\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# # Split the dataset into training (80%), validation (10%), and test (10%) sets\n",
    "# train_dataset, temp_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "# valid_dataset, test_dataset = train_test_split(temp_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00605543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_transformed.astype(np.float32))\n",
    "y_tensor = torch.tensor(y.astype(np.float32))\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split the dataset into training (80%), validation (10%), and test (10%) sets\n",
    "train_dataset, temp_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "valid_dataset, test_dataset = train_test_split(temp_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718ad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
